---
changelog:
- 2024-01-28, gpt-4-0125-preview, translated from English
date: 2024-01-28 23:57:43.361203-07:00
description: "\u041A\u0430\u043A \u044D\u0442\u043E \u0441\u0434\u0435\u043B\u0430\
  \u0442\u044C: \u041E\u0441\u043D\u043E\u0432\u043D\u043E\u0439 \u0438\u043D\u0441\
  \u0442\u0440\u0443\u043C\u0435\u043D\u0442 \u0434\u043B\u044F \u044D\u0442\u043E\
  \u0439 \u0437\u0430\u0434\u0430\u0447\u0438? `curl`. \u042D\u0442\u043E \u043C\u043E\
  \u0449\u043D\u0430\u044F \u043A\u043E\u043C\u0430\u043D\u0434\u043D\u0430\u044F\
  \ \u0443\u0442\u0438\u043B\u0438\u0442\u0430, \u043A\u043E\u0442\u043E\u0440\u0430\
  \u044F \u0438\u0437\u0432\u043B\u0435\u043A\u0430\u0435\u0442 \u0434\u0430\u043D\
  \u043D\u044B\u0435 \u0438\u0437 \u0441\u0435\u0442\u0438. \u0412\u043E\u0442 \u0441\
  \u0430\u043C\u044B\u0439 \u043F\u0440\u043E\u0441\u0442\u043E\u0439 \u043F\u0440\
  \u0438\u043C\u0435\u0440\u2026"
lastmod: '2024-03-13T22:44:45.367863-06:00'
model: gpt-4-0125-preview
summary: "\u041E\u0441\u043D\u043E\u0432\u043D\u043E\u0439 \u0438\u043D\u0441\u0442\
  \u0440\u0443\u043C\u0435\u043D\u0442 \u0434\u043B\u044F \u044D\u0442\u043E\u0439\
  \ \u0437\u0430\u0434\u0430\u0447\u0438."
title: "\u0417\u0430\u0433\u0440\u0443\u0437\u043A\u0430 \u0432\u0435\u0431-\u0441\
  \u0442\u0440\u0430\u043D\u0438\u0446\u044B"
weight: 42
---

## Как это сделать:
Основной инструмент для этой задачи? `curl`. Это мощная командная утилита, которая извлекает данные из сети. Вот самый простой пример использования:

```Bash
curl https://example.com -o webpage.html
```

Эта команда скачивает HTML с `example.com` и записывает его в файл под названием `webpage.html`. Посмотрите на вывод:

```Bash
# Пример вывода
  % Всего    % Получено % Передано  Средняя скорость   Время    Время     Время  Текущее
                                 Скач.  Отпр.   Всего   Потрач.    Ост.  Скорость
100  1256  100  1256    0     0   6458      0 --:--:-- --:--:-- --:--:--  6497
```

Хотите видеть, что вы скачиваете в реальном времени? Уберите `-o`, и скачивание отобразится прямо в вашей консоли:

```Bash
curl https://example.com
```

## Погружение в детали
`curl` существует с 1997 года, завоевав свою нишу в веб-операциях. Почему `curl`, а не скачивание через браузер? Автоматизация и дружелюбие к скриптам. Он неинтерактивен и может быть легко интегрирован в bash-скрипты.

Стоит упомянуть альтернативы: `wget`, другая мощная командная утилита, которая может рекурсивно скачивать веб-страницы. Для серьёзного скрапинга или когда необходим контекст реального браузера, программисты обращаются к инструментам вроде Selenium, Puppeteer или Scrapy.

Изучение работы `curl`: Он поддерживает множество протоколов, от HTTP и HTTPS до FTP, и множество опций (--header, --cookie, --user-agent и т. д.) для настройки запросов. Плюс, обычно он уже предустановлен на системах на базе Unix.

## Смотрите также
- Документация Curl: https://curl.haxx.se/docs/manpage.html
- Руководство Wget: https://www.gnu.org/software/wget/manual/wget.html
- Введение в веб-скрапинг на Python: https://realpython.com/python-web-scraping-practical-introduction/
