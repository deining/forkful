---
changelog:
- 2024-01-28, gpt-4-0125-preview, translated from English
date: 2024-01-28 23:57:45.981416-07:00
description: "\u041A\u0430\u043A \u044D\u0442\u043E \u0441\u0434\u0435\u043B\u0430\
  \u0442\u044C: \u041C\u044B \u0431\u0443\u0434\u0435\u043C \u0438\u0441\u043F\u043E\
  \u043B\u044C\u0437\u043E\u0432\u0430\u0442\u044C \u0431\u0438\u0431\u043B\u0438\u043E\
  \u0442\u0435\u043A\u0443 `requests` \u0434\u043B\u044F Python. \u0415\u0441\u043B\
  \u0438 \u0443 \u0432\u0430\u0441 \u0435\u0435 \u043D\u0435\u0442, \u0443\u0441\u0442\
  \u0430\u043D\u043E\u0432\u0438\u0442\u0435 \u0435\u0435 \u0441 \u043F\u043E\u043C\
  \u043E\u0449\u044C\u044E `pip install requests`. \u0412\u043E\u0442 \u0431\u044B\
  \u0441\u0442\u0440\u044B\u0439\u2026"
lastmod: '2024-03-13T22:44:44.267856-06:00'
model: gpt-4-0125-preview
summary: "\u041C\u044B \u0431\u0443\u0434\u0435\u043C \u0438\u0441\u043F\u043E\u043B\
  \u044C\u0437\u043E\u0432\u0430\u0442\u044C \u0431\u0438\u0431\u043B\u0438\u043E\u0442\
  \u0435\u043A\u0443 `requests` \u0434\u043B\u044F Python."
title: "\u0417\u0430\u0433\u0440\u0443\u0437\u043A\u0430 \u0432\u0435\u0431-\u0441\
  \u0442\u0440\u0430\u043D\u0438\u0446\u044B"
weight: 42
---

## Как это сделать:
Мы будем использовать библиотеку `requests` для Python. Если у вас ее нет, установите ее с помощью `pip install requests`. Вот быстрый пример:

```python
import requests

url = 'https://www.example.com'
response = requests.get(url)

if response.ok:
    html_content = response.text
    print(html_content)
else:
    print("Не удалось получить веб-страницу")

```

Когда этот скрипт запускается, если все прошло успешно, вы увидите в консоли HTML-содержимое "https://www.example.com".

## Подробный анализ
До `requests`, в Python был `urllib`. Он все еще существует, но `requests` затмил его своим простым, дружественным интерфейсом. `requests` был выпущен в 2011 году Кеннетом Рейтцем и с тех пор стал золотым стандартом для работы с HTTP в Python. Но дело не только в простоте - `requests` также является надежным, предлагая такие функции, как объекты сеанса, сохранение cookies и автоматическую обработку SSL-сертификатов.

Существуют альтернативы, такие как `http.client`, которая находится на более низком уровне, чем `requests`, и внешние библиотеки, такие как `aiohttp` для асинхронных операций. Глубоко под капотом, независимо от вашего выбора, эти библиотеки взаимодействуют с веб-серверами, отправляют HTTP-запросы и обрабатывают ответы.

Когда вы скачиваете страницы, важно учитывать правила дороги: уважайте файлы `robots.txt`, чтобы знать, где вам разрешено, и не перегружайте серверы - замедляйте ваши запросы. Также имейте в виду, что веб-страницы могут загружать динамическое содержимое с помощью JavaScript, которое не будет захвачено простым HTTP-запросом.

## Смотрите также:
- документация по `requests`: https://requests.readthedocs.io/en/master/
- информация о `urllib`: https://docs.python.org/3/library/urllib.html
- введение в `robots.txt`: https://www.robotstxt.org
- `aiohttp` для асинхронных веб-запросов: https://docs.aiohttp.org/en/stable/
